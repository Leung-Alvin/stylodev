Artificial intelligence has become a very popular computing industry in the recent decade [2]. The concept of humanless yet humanlike decision making has become integral to several industries such as financial technology, medicine, business, etc. However, one segment of artificial intelligence has particularly high amounts of controversy: self-driving vehicles. Self-driving cars are cars that are embedded with artificial intelligence, allowing them to drive through complex situations with humanlike judgment without human guidance. The problem concerning them is when human life is on the line and the potential business-oriented bias that may come up in possibly fatal decision-making. There is almost no precaution provided as AI is a novel concept with little regulation, which is why AI is the center of such controversy.


The Moral Machine is a massive online survey that presented participants with moral dilemmas involving automated vehicles and choices surrounding human life. In particular, it was shown that ethics varied based on participant information and country [1]. For example, a majority of pollers would protect pedestrians over passengers. However this is not a favorable business model. This business model could potentially harm the car industry and favors manufacturing cars that protect pedestrians over passengers [1]. Few would purchase cars that do not protect the customers. This ultimately leads to business-oriented bias, where businesses may make unethical decisions in order to protect or further their business. This bias is directly involved with computing. Self-driving mechanisms are written by humans, which may include these biases. Programmers in general follow what is profitable for a company as that is their livelihood. This can cause AI programs to be intentionally written with unethical biases. Cars may be programmed to kill pedestrians if the result would be to protect the passengers. Ultimately, AI could harm the public if profits are placed above safety [2]. 


One way that self-driving cars have been regulated is through the National Highway Transportation Safety Association (NHTSA) [2]. The NHTSA has a small, specialized personnel group that specifically deals with self-driving cars and understands their inner workings well enough to provide some level of regulation in order to prevent large-scale issues. The solution at the moment would be to likely further regulate self-driving mechanisms. Presently, the government barely regulates, if at all, how companies use AI in their business models. On the other hand, companies are also not held accountable for potential damages that their AIs cause either [2]. Because of this, the solution ultimately is to regulate the companies, which in turn makes companies responsible for unethical AI practices. This incentivizes companies to make socially acceptable AIs and to protect the general public.


[1]        A. Maxmen, “Self-driving car dilemmas reveal that moral choices are not universal,” Nature News, 24-Oct-2018. [Online]. Available: https://www.nature.com/articles/d41586-018-07135-0. [Accessed: 05-May-2023]. 


[2]         C. Pazzanese, “Ethical concerns mount as AI takes bigger decision-making role,” Harvard Gazette, 04-Dec-2020. [Online]. Available: https://news.harvard.edu/gazette/story/2020/10/ethical-concerns-mount-as-ai-takes-bigger-decision-making-role/. [Accessed: 05-May-2023].